<p align="center" style="margin: 0; padding: 0;">
  <img 
    src="https://github.com/KartikSaroop-AI/DailyEpoch/blob/main/dailyepoch.png.png"
    alt="DailyEpoch Banner"
    width="1000"
    height="300"
    style="display: block; object-fit: cover; border-radius: 10px; box-shadow: 0 3px 10px rgba(0,0,0,0.2);"
  />
</p>

<h1 align="center">ğŸ—“ï¸ DailyEpoch</h1>
<p align="center">A daily research journal capturing my evolving journey through Machine Learning, Deep Learning, Artificial Intelligence, Python, and Large Language Models â€” one gradient step at a time.</p>

<p align="center">
  <!-- Line 1: Academic / Research Topics -->
  <img src="https://img.shields.io/badge/Machine%20Learning-Research-blue?style=for-the-badge">
  <img src="https://img.shields.io/badge/Deep%20Learning-Architectures-orange?style=for-the-badge">
  <img src="https://img.shields.io/badge/Artificial%20Intelligence-Theory%20%7C%20Practice-red?style=for-the-badge">
  <img src="https://img.shields.io/badge/Large%20Language%20Models-Prompting%20%7C%20Fine--Tuning-purple?style=for-the-badge">
  <img src="https://img.shields.io/badge/Mathematics-Linear%20Algebra%20%7C%20Calculus-yellow?style=for-the-badge">
</p>

<p align="center">
  <!-- Line 2: Tools & Frameworks -->
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white">
  <img src="https://img.shields.io/badge/Jupyter-Notebook-F37626?style=for-the-badge&logo=jupyter">
  <img src="https://img.shields.io/badge/Scikit--Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white">
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white">
  <img src="https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white">
  <img src="https://img.shields.io/badge/Matplotlib-005C5C?style=for-the-badge&logo=plotly&logoColor=white">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white">
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white">
</p>


---
## ğŸ§­ Daily Logs â€” AI Learning Chronicle  
> *â€œRecording daily gradients of intelligence and discovery.â€*

---

### ğŸ“… **2025-11-09**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Generative Adversarial Networks (GANs)** | Adversarial Learning, Generatorâ€“Discriminator, Minimax Optimization, Latent Space | GANs consist of two competing networks â€” a Generator that creates data and a Discriminator that distinguishes real from fake. Through adversarial training, both improve iteratively, enabling realistic data synthesis. | [ğŸ“˜ GAN Notes (PDF)](Docs/GAN.pdf) |

---

### ğŸ“… **2025-11-11**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Convolutional Neural Networks (CNNs)** | Convolution, Padding, Pooling, Feature Maps, Spatial Hierarchies | CNNs extract hierarchical features from images, preserve details through padding, and enhance efficiency via pooling â€” foundational to modern computer vision. | [ğŸ“˜ CNN Overview](Docs/CNN.pdf) Â· [ğŸ“˜ Padding](Docs/padding.pdf) Â· [ğŸ“˜ Pooling Layers](Docs/Poolinglayers.pdf) |

---

### ğŸ“… **2025-11-12**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Recurrent Neural Networks (RNNs) & LSTMs** | Sequential Data, Temporal Dependencies, Gradient Flow, Memory Cells | RNNs process sequential information, while LSTMs overcome vanishing gradients by introducing gates that control information flow for long-term memory. | [ğŸ“˜ RNN_LSTM.pdf](Docs/RNN_LSTM.pdf) |

---

### ğŸ“… **2025-11-13**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Transformers & Attention Mechanisms** | Self-Attention, Multi-Head Attention, Positional Encoding | Transformers revolutionize sequence modeling by replacing recurrence with attention â€” allowing global context understanding and parallel computation. | [ğŸ“˜ Transformers.pdf](Docs/Transformers_Attention.pdf) |

---

### ğŸ“… **2025-11-14**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Large Language Models (LLMs)** | Context Windows, Tokenization, Emergent Behavior, Pretraining | LLMs leverage massive transformer architectures to learn language representations and exhibit emergent reasoning from large-scale data. | [ğŸ“˜ LLMs_Fundamentals.pdf](Docs/LLMs_Fundamentals.pdf) |

---

### ğŸ“… **2025-11-15**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Natural Language Processing (NLP) Basics** | Tokenization, Lemmatization, Text Vectorization | NLP enables machines to interpret language through preprocessing, feature extraction, and semantic representation. | [ğŸ“˜ NLP_Basics.pdf](Docs/NLP_Basics.pdf) |

---

### ğŸ“… **2025-11-16**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Word Embeddings (Word2Vec & GloVe)** | Distributed Representation, Context Windows, Semantic Similarity | Word embeddings capture semantic meaning by mapping words into dense vector spaces, enabling contextual understanding for downstream NLP tasks. | [ğŸ“˜ Word2Vec.pdf](Docs/Word2Vec.pdf) |

---

### ğŸ“… **2025-11-17**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Dimensionality Reduction (PCA, t-SNE)** | Variance, Eigenvectors, Manifold Learning | Dimensionality reduction simplifies complex data while preserving structure, aiding visualization and feature compression. | [ğŸ“˜ Dimensionality_Reduction.pdf](Docs/Dimensionality_Reduction.pdf) |

---

### ğŸ“… **2025-11-18**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Feature Engineering & Selection** | Normalization, Encoding, Feature Importance | Proper feature design improves model interpretability and performance by emphasizing relevant variables. | [ğŸ“˜ Feature_Engineering.pdf](Docs/Feature_Engineering.pdf) |

---

### ğŸ“… **2025-11-19**

| **Topic** | **Key Concepts** | **Summary** | **Artifacts** |
|:-----------|:-----------------|:-------------|:---------------|
| **Regression Models (Linear & Logistic)** | Loss Minimization, Sigmoid, Gradient Descent | Regression models establish statistical relationships; logistic regression extends this for binary classification through non-linear mapping. | [ğŸ“˜ Regression_Models.pdf](Docs/Regression_Models.pdf) |

---

> ğŸ§© *â€œEvery day logged is another gradient in the space of understanding.â€*
